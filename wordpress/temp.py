#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
python temp.py 
  --sheet-id "1IHbEqLDqA1S-dmweDdl1jM8OeuXbvkXcevMYK3DGsao" 
  --range "'ç¨¼ã’ãªã„'!A2:E" 
  --col-keywords 1 
  --col-title 2 
  --col-url 3 
  --col-docurl 4 
  --col-flag 5 
  --ok-token "OK" 
  --outdir "kasegenai3"
"""

import argparse
import os
import re
import sys
import unicodedata
from typing import Dict, List, Optional, Tuple

from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import pickle

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets.readonly",
    "https://www.googleapis.com/auth/documents.readonly",
    "https://www.googleapis.com/auth/drive.readonly",
]

# =========================
# Helpers
# =========================

DOC_ID_RE = re.compile(r"/document/d/([a-zA-Z0-9_-]+)")

def extract_doc_id(doc_url: str) -> Optional[str]:
    if not doc_url:
        return None
    m = DOC_ID_RE.search(doc_url)
    return m.group(1) if m else None

def slugify(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = text.strip().lower()
    # æ‹¬å¼§ã‚„è¨˜å·ã‚’ãƒ€ãƒƒã‚·ãƒ¥ã¸
    text = re.sub(r"[^\w\-]+", "-", text)
    text = re.sub(r"-{2,}", "-", text)
    text = text.strip("-_")
    return text or "untitled"

def ensure_unique_path(base_dir: str, base_name: str, ext: str = ".md") -> str:
    os.makedirs(base_dir, exist_ok=True)
    path = os.path.join(base_dir, base_name + ext)
    if not os.path.exists(path):
        return path
    i = 2
    while True:
        p = os.path.join(base_dir, f"{base_name}-{i}{ext}")
        if not os.path.exists(p):
            return p
        i += 1

def escape_md(text: str) -> str:
    # æœ€ä½é™ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ï¼ˆå¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µï¼‰
    return text.replace("\\", "\\\\").replace("*", "\\*").replace("_", "\\_").replace("`", "\\`")

def is_ok_row(row: List[str], col_flag: int, ok_token: str) -> bool:
    """
    æŒ‡å®šåˆ—ãŒ OK ã‹ã©ã†ã‹ï¼ˆ1å§‹ã¾ã‚Šã®åˆ—ç•ªå·ã‚’å—ã‘å–ã‚Šã€å¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ã—ã¦æ¯”è¼ƒï¼‰
    """
    i = col_flag - 1  # 1å§‹ã¾ã‚Šâ†’0å§‹ã¾ã‚Š
    if i >= len(row):
        return False
    return row[i].strip().upper() == ok_token.strip().upper()

# =========================
# Google API Clientsï¼ˆInstalledAppFlowï¼‰
# =========================

def get_clients():
    # SCOPES ã¯æ—¢å­˜ã®é…åˆ—ã‚’ãã®ã¾ã¾åˆ©ç”¨
    creds = None
    token_path = "token.pickle"  # æ‰¿èªå¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜

    if os.path.exists(token_path):
        with open(token_path, "rb") as f:
            creds = pickle.load(f)

    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            # ã‚«ãƒ¬ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã® credentials.json ã‚’åˆ©ç”¨
            flow = InstalledAppFlow.from_client_secrets_file("credentials.json", SCOPES)
            creds = flow.run_local_server(port=0)
        with open(token_path, "wb") as f:
            pickle.dump(creds, f)

    sheets = build("sheets", "v4", credentials=creds)
    docs   = build("docs",   "v1", credentials=creds)
    drive  = build("drive",  "v3", credentials=creds)
    return sheets, docs, drive

# =========================
# Docs â†’ Markdown å¤‰æ›
# =========================

def text_run_to_md(tr: Dict) -> str:
    """
    Docs API: ParagraphElement.textRun ã‚’ Markdown ã«å¤‰æ›
    - åˆ¶å¾¡æ–‡å­— (VTãªã© [\x00-\x1F]) ã¯ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ã—ã¦ã‚´ãƒŸæ··å…¥ã‚’é˜²æ­¢
    """
    if "content" not in tr:
        return ""
    txt = tr["content"]
    # åˆ¶å¾¡æ–‡å­—ï¼ˆvtç­‰ï¼‰ã‚’é™¤å»
    txt = re.sub(r"[\x00-\x1F]+", " ", txt)
    style = (tr.get("textStyle") or {})
    link = (style.get("link") or {}).get("url")
    bold = style.get("bold", False)
    italic = style.get("italic", False)
    code = style.get("code", False)

    # è¡Œæœ«ã®å˜ç‹¬æ”¹è¡Œã‚’ç¶­æŒï¼ˆDocs ã¯è¦ç´ å˜ä½ã§æ”¹è¡Œå«ã‚€ã“ã¨ãŒå¤šã„ï¼‰
    txt = txt.replace("\r", "")

    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è£…é£¾
    out = txt
    if link:
        # ãƒªãƒ³ã‚¯å„ªå…ˆï¼ˆè£…é£¾ã¯ä¸­ã«å«ã‚ãªã„ç°¡æ˜“å®Ÿè£…ï¼‰
        out = f"[{out.strip()}]({link})"
    else:
        if code:
            out = f"`{out.strip()}`"
        if bold and italic:
            out = f"***{out.strip()}***"
        elif bold:
            out = f"**{out.strip()}**"
        elif italic:
            out = f"*{out.strip()}*"

    return out

def paragraph_to_md(p: Dict, lists_meta: Dict) -> str:
    """
    Docs API: Paragraph ã‚’ Markdown ã«å¤‰æ›
    - è¦‹å‡ºã—: HEADING_1..6
    - ç®‡æ¡æ›¸ã: bullet ãŒã‚ã‚Œã°ã€Œãƒ»ã€ã«çµ±ä¸€ï¼ˆordered/unordered å•ã‚ãšï¼‰
    - é€šå¸¸æ®µè½
    """
    pstyle = (p.get("paragraphStyle") or {})
    named = pstyle.get("namedStyleType", "NORMAL_TEXT")

    # ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã®é€£çµ
    texts: List[str] = []
    for el in p.get("elements", []):
        tr = el.get("textRun")
        if tr:
            texts.append(text_run_to_md(tr))
    content = "".join(texts).rstrip("\n")

    # è¦‹å‡ºã—
    if named.startswith("HEADING_"):
        try:
            level = int(named.split("_", 1)[1])
        except:
            level = 2
        level = min(max(level, 1), 6)
        return f"{'#' * level} {content}\n"

    # ç®‡æ¡æ›¸ãï¼ˆordered/unordered ã«é–¢ã‚ã‚‰ãšã€Œãƒ»ã€ã‚’ä½¿ç”¨ï¼‰
    bullet = p.get("bullet")
    if bullet:
        nesting = 0
        if bullet.get("nestingLevel") is not None:
            nesting = bullet["nestingLevel"]
        indent = "  " * nesting
        prefix = "ãƒ»"  # â†ã”è¦æœ›ã«åˆã‚ã›ã¦å›ºå®š
        line = f"{indent}{prefix} {content}".rstrip()
        return f"{line}\n"

    # é€šå¸¸æ®µè½
    if content.strip() == "":
        return "\n"
    return f"{content}\n"

def analyze_lists(document: Dict) -> Dict[str, Dict]:
    """
    Docs ã® listId -> ordered/unordered ã®ç°¡æ˜“åˆ¤å®š
    ï¼ˆä»Šå›ã¯ prefix ã‚’å¸¸ã«ã€Œãƒ»ã€ã«ã™ã‚‹ãŸã‚ã€å‚ç…§ã®ã¿ã§å®Ÿè³ªæœªä½¿ç”¨ï¼‰
    """
    meta: Dict[str, Dict] = {}
    for lid, props in (document.get("lists") or {}).items():
        glyph = (props.get("listProperties") or {}).get("nestingLevels", [])
        ordered = False
        if glyph:
            if any("glyphType" in g for g in glyph):
                ordered = True
        meta[lid] = {"ordered": ordered}
    return meta

def document_to_markdown(document: Dict) -> str:
    """
    Google Docs ã®æ§‹é€ ã‚’è¦‹ã¦ Markdown æ–‡å­—åˆ—ã«å¤‰æ›
    """
    body = (document.get("body") or {}).get("content", [])
    lists_meta = analyze_lists(document)

    lines: List[str] = []
    for el in body:
        if "horizontalRule" in el:
            continue
        p = el.get("paragraph")
        if p:
            lines.append(paragraph_to_md(p, lists_meta))
        # è¡¨ãƒ»ç”»åƒãƒ»å›³å½¢ãªã©ã¯ä»Šå›ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µï¼‰

    # æœ«å°¾ã®ä½™è¨ˆãªç©ºè¡Œã‚’æ•´ç†
    md = "".join(lines)
    md = re.sub(r"\n{3,}", "\n\n", md).strip() + "\n"
    return md

# =========================
# ã‚·ãƒ¼ãƒˆèª­ã¿å–ã‚Š & æ›¸ãå‡ºã—
# =========================

def read_sheet_rows(sheets, sheet_id: str, range_a1: str) -> List[List[str]]:
    res = sheets.spreadsheets().values().get(spreadsheetId=sheet_id, range=range_a1).execute()
    return res.get("values", [])

def fetch_document(docs, doc_id: str) -> Dict:
    return docs.documents().get(documentId=doc_id).execute()

def build_front_matter(title: str, src_url: str, doc_url: str) -> str:
    """
    å…ˆé ­ã« H1 ã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ä¸ã€‚
    - source_url ã¯å€¤ãŒç©ºãªã‚‰å‡ºåŠ›ã—ãªã„ï¼ˆç©ºã‚³ãƒ¡ãƒ³ãƒˆè¡ŒãŒå‡ºãªã„ã‚ˆã†ã«ï¼‰
    """
    fm = []
    if title:
        fm.append(f"# {title}\n")
    if src_url:
        fm.append(f"<!-- source_url: {src_url} -->\n")
    fm.append(f"<!-- gdoc_url:  {doc_url or ''} -->\n\n")
    return "".join(fm)

def remove_tail_cta(md: str) -> str:
    """
    æœ«å°¾ã«å…¥ã£ã¦ã„ã‚‹LINE/ãƒ¡ãƒ«ãƒã‚¬èª˜å°ã®å‘ŠçŸ¥ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    """
    md = re.sub(
        r"(?s)\n?[ \t]*åƒ•ã¯LINEã¨ãƒ¡ãƒ«ãƒã‚¬ã‚’ã‚„ã£ã¦ã„ã¦.*\Z",
        "\n",
        md,
        flags=re.MULTILINE,
    )
    patterns = [
        r"(?m)^[ \t]*ğŸ‘‰.*\n?",                 # ğŸ‘‰ã§å§‹ã¾ã‚‹è¡Œ
        r"(?m)^.*å…¬å¼LINE.*\n?",              # å…¬å¼LINE ã®è¡Œ
        r"(?m)^.*ãƒ¡ãƒ«ãƒã‚¬.*\n?",              # ãƒ¡ãƒ«ãƒã‚¬ ã®è¡Œ
        r"(?m)^.*line\.me/R/ti/p/%40dxw9105c.*\n?",            # LINE URL
        r"(?m)^.*ãƒãƒƒã‚¯ãƒ‘ãƒƒã‚«ãƒ¼\.jp/manga/?\S*.*\n?",          # ãƒ¡ãƒ«ãƒã‚¬URL
    ]
    for pat in patterns:
        md = re.sub(pat, "", md)

    md = re.sub(r"\n{3,}", "\n\n", md).strip() + "\n"
    return md

def process_row(
    docs,
    row: List[str],
    idx: int,
    col_keywords: int,
    col_title: int,
    col_url: int,
    col_docurl: int,
    outdir: str,
) -> Optional[str]:
    """
    1 è¡Œå‡¦ç†ã—ã¦ MD ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚æˆåŠŸã—ãŸã‚‰ãƒ‘ã‚¹ã‚’è¿”ã™
    """
    def safe_get(col: int) -> str:
        # 1å§‹ã¾ã‚Š â†’ 0-based
        i = col - 1
        return row[i].strip() if (0 <= i < len(row)) else ""

    keywords = safe_get(col_keywords)
    title = safe_get(col_title) or keywords or f"no-title-{idx}"
    url = safe_get(col_url)
    doc_url = safe_get(col_docurl)
    doc_id = extract_doc_id(doc_url)
    if not doc_id:
        print(f"[warn] row {idx}: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆURLä¸æ­£ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— ({doc_url})", file=sys.stderr)
        return None

    try:
        document = fetch_document(docs, doc_id)
    except HttpError as e:
        print(f"[error] row {idx}: Docså–å¾—ã«å¤±æ•— docId={doc_id} {e}", file=sys.stderr)
        return None

    body_md = document_to_markdown(document)
    body_md = remove_tail_cta(body_md)
    head = build_front_matter(title, url, doc_url)

    base_name = slugify(title)
    out_path = ensure_unique_path(outdir, base_name, ".md")
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(head)
        f.write(body_md)

    print(f"[ok]  row {idx}: {out_path}")
    return out_path

def main():
    ap = argparse.ArgumentParser(description="Google Docs â†’ Markdown exporter (from Google Sheets rows)")
    ap.add_argument("--sheet-id", required=True, help="ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID")
    ap.add_argument("--range", required=True, help="èª­ã¿å–ã‚Šç¯„å›²ï¼ˆä¾‹: 'Articles!A2:E'ï¼‰")
    ap.add_argument("--col-keywords", type=int, default=1, help="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ åˆ—ç•ªå·(1å§‹ã¾ã‚Š)")
    ap.add_argument("--col-title", type=int, default=2, help="è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« åˆ—ç•ªå·(1å§‹ã¾ã‚Š)")
    ap.add_argument("--col-url", type=int, default=3, help="URL åˆ—ç•ªå·(1å§‹ã¾ã‚Š)")
    ap.add_argument("--col-docurl", type=int, default=4, help="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆURL åˆ—ç•ªå·(1å§‹ã¾ã‚Š)")
    ap.add_argument("--outdir", default="kasegenai", help="å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€")
    # â˜…è¿½åŠ : ãƒ•ã‚£ãƒ«ã‚¿åˆ—ã¨ãƒˆãƒ¼ã‚¯ãƒ³
    ap.add_argument("--col-flag", type=int, default=5, help="å¤‰æ›ãƒ•ãƒ©ã‚°åˆ—ç•ªå·(1å§‹ã¾ã‚Š) ä¾‹: Eåˆ—=5")
    ap.add_argument("--ok-token", default="OK", help="å¤‰æ›å¯¾è±¡ã¨ã¿ãªã™ã‚»ãƒ«ã®å€¤ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–ï¼‰")
    args = ap.parse_args()

    try:
        sheets, docs, drive = get_clients()
    except Exception as e:
        print(f"[fatal] Google API èªè¨¼/åˆæœŸåŒ–ã«å¤±æ•—: {e}", file=sys.stderr)
        sys.exit(1)

    rows = read_sheet_rows(sheets, args.sheet_id, args.range)
    if not rows:
        print("[info] ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    count = 0
    for i, row in enumerate(rows, start=1):
        # â˜… Eåˆ—(æ—¢å®š)ãŒ OK ã®è¡Œã ã‘å‡¦ç†
        if not is_ok_row(row, args.col_flag, args.ok_token):
            print(f"[skip] row {i}: ãƒ•ãƒ©ã‚°åˆ—ãŒOKã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue

        p = process_row(
            docs,
            row,
            idx=i,
            col_keywords=args.col_keywords,
            col_title=args.col_title,
            col_url=args.col_url,
            col_docurl=args.col_docurl,
            outdir=args.outdir,
        )
        if p:
            count += 1

    print(f"[done] {count} ä»¶ã® Markdown ã‚’ {args.outdir}/ ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
